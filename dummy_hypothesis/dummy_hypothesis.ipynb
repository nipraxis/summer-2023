{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad84cc63",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c566b340",
   "metadata": {},
   "source": [
    "## Introduction and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Import numerical and plotting libraries\n",
    "import numpy as np\n",
    "# Print to four digits of precision\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import numpy.linalg as npl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b864a26",
   "metadata": {},
   "source": [
    "This exercise returns to the psychopathy of students from Berkeley and MIT.  It\n",
    "continues from the `on_dummies` exercise.  Make sure you have done that\n",
    "exercise before doing this one.\n",
    "\n",
    "Here are the psychopathy questionnaire scores from another set of 5 students\n",
    "from Berkeley and MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_psycho = np.array([2.9277, 9.7348, 12.1932, 12.2576, 5.4834])\n",
    "n_ucb = len(ucb_psycho)\n",
    "mit_psycho = np.array([7.2937, 11.1465, 13.5204, 15.053, 12.6863])\n",
    "n_mit = len(mit_psycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534decce",
   "metadata": {},
   "source": [
    "$\\newcommand{\\yvec}{\\vec{y}}$\n",
    "\n",
    "The `psychopathy` values will be our `y` vector $\\yvec$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give name y to psychopathy, for reading convenience.\n",
    "y = np.concatenate([ucb_psycho, mit_psycho])\n",
    "# Call the number of observations \"n\"\n",
    "n = len(y)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb40e10",
   "metadata": {},
   "source": [
    "We  use the general linear model to do a two-level (UCB, MIT) single factor\n",
    "(college) analysis of variance on these data.\n",
    "\n",
    "Our model is that the Berkeley student data are drawn from some distribution\n",
    "with a mean value that is characteristic for Berkeley: $y_i = \\mu_{Berkeley} +\n",
    "e_i$ where $i$ corresponds to a student from Berkeley.  There is also a\n",
    "characteristic but possibly different mean value for MIT: $\\mu_{MIT}$:\n",
    "\n",
    "$$\n",
    "\\newcommand{\\xvec}{\\vec{x}}\n",
    "\\newcommand{\\evec}{\\vec{\\varepsilon}}\n",
    "\\newcommand{\\Xmat}{\\boldsymbol X}\n",
    "\\newcommand{\\bvec}{\\vec{\\beta}}\n",
    "\\newcommand{\\bhat}{\\hat{\\bvec}}\n",
    "\\newcommand{\\yhat}{\\hat{\\yvec}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i = \\mu_{Berkeley} + e_i  \\space\\mbox{if}\\space 1 \\le i \\le 5 \\\\\n",
    "y_i = \\mu_{MIT} + e_i \\space\\mbox{if}\\space 6 \\le i \\le 10\n",
    "$$\n",
    "\n",
    "Here is the design matrix for this ANOVA, with dummy variables corresponding to\n",
    "the UCB and MIT student groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Create design matrix for UCB / MIT ANOVA\n",
    "X = ...\n",
    "# Show the result\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7be35",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "assert X.shape == (n, 2)\n",
    "assert np.all(np.sum(X, axis=0) == [n_ucb, n_mit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06cc82d",
   "metadata": {},
   "source": [
    "The betas are given by:\n",
    "\n",
    "$$\n",
    "\\bhat = (\\Xmat^T \\Xmat)^{-1}\\Xmat^T \\yvec\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2865697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate transpose of design matrix multiplied by data, and therefore\n",
    "#- calculate beta vector\n",
    "B = ...\n",
    "# Show the result\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result is as expected, the means of each group.\n",
    "assert B.shape == (2,)\n",
    "assert np.allclose(B, [np.mean(y[:n_ucb]), np.mean(y[n_ucb:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325ece0",
   "metadata": {},
   "source": [
    "## Hypothesis testing with contrasts\n",
    "\n",
    "Remember the student’s t statistic from the general linear model :\n",
    "\n",
    "$$\n",
    "\\newcommand{\\cvec}{\\vec{c}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{\\cvec^T \\bhat}\n",
    "{\\sqrt{\\hat{\\sigma}^2 \\cvec^T (\\Xmat^T \\Xmat)^+ \\cvec}}\n",
    "$$\n",
    "\n",
    "Let’s consider the top half of the t statistic, $c^T \\bhat$.\n",
    "\n",
    "Our hypothesis is that the mean psychopathy score for MIT students,\n",
    "$\\mu_{MIT}$, is higher than the mean psychopathy score for Berkeley students,\n",
    "$\\mu_{Berkeley}$.  What contrast vector $\\cvec$ do we need to apply to $\\bhat$\n",
    "to express the difference between these means?  Apply this contrast vector to\n",
    "$\\bhat$ to get the top half of the t statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Contrast vector to express difference between UCB and MIT\n",
    "#- Resulting value will be high and positive when MIT students have\n",
    "#- higher psychopathy scores than UCB students\n",
    "c =\n",
    "top_of_t ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4183ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert top_of_t > 0, 'Oops, did you subtract the wrong value?'\n",
    "assert np.isclose(top_of_t, np.mean(mit_psycho) - np.mean(ucb_psycho))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004ae86",
   "metadata": {},
   "source": [
    "Now the bottom half of the t statistic.  Remember this is\n",
    "$\\sqrt{\\hat{\\sigma}^2 \\cvec^T (\\Xmat^T \\Xmat)^+ \\cvec}$.\n",
    "\n",
    "First we generate $\\hat{\\sigma^2}$ from the residuals of the model.\n",
    "\n",
    "Calculate the fitted values and the residuals given the $\\bhat$ that you have\n",
    "already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47796cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the fitted and residual values\n",
    "fitted = ...\n",
    "residuals = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1430280",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(residuals) == n\n",
    "assert np.isclose(np.mean(residuals), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bdf22",
   "metadata": {},
   "source": [
    "**For reflection**: Notice we tested that the mean of the residuals was close-as-dammit to 0.  How did we know that would be true?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4787ddd",
   "metadata": {},
   "source": [
    "We want an unbiased variance estimate for $\\hat\\sigma^2$.  See the [worked\n",
    "example of GLM](https://textbook.nipraxis.org/mean_test_example.html) page and the [unbiased variance estimate](https://textbook.nipraxis.org/hypothesis_tests.html#unbiased-variance) section for\n",
    "details.\n",
    "\n",
    "The general rule is that we divide the sum of squares by $n - m$ where $m$ is\n",
    "the number of *independent* columns in the design matrix.  Specifically, $m$\n",
    "is the [matrix rank](http://matthew-brett.github.io/teaching/matrix_rank.html) of the design $\\Xmat$.  $m$ can also be called the\n",
    "*degrees of freedom of the design*.  $n - m$ is the *degrees of freedom of the\n",
    "error* (see [unbiased variance estimate](https://textbook.nipraxis.org/hypothesis_tests.html#unbiased-variance))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the degrees of freedom consumed by the design.\n",
    "m = ...\n",
    "#- Calculate the degrees of freedom of the error.\n",
    "df_error = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c48c0",
   "metadata": {},
   "source": [
    "Calculate the unbiased *variance* estimate $\\hat{\\sigma^2}$ by dividing the\n",
    "sums of squares of the residuals by the degrees of freedom of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the unbiased variance estimate\n",
    "var_hat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(var_hat, 3) == 13.049"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1d17c",
   "metadata": {},
   "source": [
    "Now the calculate second part of the t statistic denominator,  $\\cvec^T (\\Xmat^T\n",
    "\\Xmat)^+ \\cvec$. You already know that $\\Xmat^T \\Xmat$ is invertible, and you\n",
    "have its inverse above, so you can use the inverse instead of the more general\n",
    "pseudo-inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aea971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate c (X.T X)^-1 c.T\n",
    "c_iXtX_ct = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(c_iXtX_ct, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946300c",
   "metadata": {},
   "source": [
    "The final deminator of the t-statistic is the square root of the estimated variance `var_hat` mutiplied by the second half you have just calculated. The t-statistic is the numerator divided by the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e377f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate t-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(t_stat, 3) == 1.497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6e9b8",
   "metadata": {},
   "source": [
    "How likely is a t-statistic value this positive, or more positive, if there was in fact no underlying difference between the groups ? Use the `stats` module from `scipy` to create a\n",
    "t-distribution with `df_error` (degrees of freedom of the error).  See the\n",
    "`t_stat` function in [introduction to the general linear model](https://matthew-brett.github.io/teaching/glm_intro.html) for\n",
    "inspiration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32371530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Use scipy.stats to give a probability of a value as positive as the one you observe.\n",
    "import scipy.stats as sst\n",
    "p_val = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8295c",
   "metadata": {},
   "source": [
    "Check your result against the Scipy implementation of the independent t-test.\n",
    "\n",
    "Scipy, by default, calculates the two-tailed p-value, whereas you have calculated the one-sided value.  Scipy's p value should be very close to 2 x your p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_result = sst.ttest_ind(mit_psycho, ucb_psycho)\n",
    "t_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(t_test_result.statistic, t_stat)\n",
    "assert np.isclose(t_test_result.pvalue, p_val * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bde91",
   "metadata": {},
   "source": [
    "## Advanced hypothesis testing: F-tests\n",
    "\n",
    "For this part of the exercise, you might consider reading [hypothesis tests\n",
    "with the GLM](https://textbook.nipraxis.org/hypothesis_tests.html).\n",
    "\n",
    "Imagine we have also measured the clammy score for the Berkeley and MIT\n",
    "students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Clamminess of handshake for UCB and MIT students\n",
    "clammy = np.array([2.6386, 9.6094, 8.3379, 6.2871, 7.2775, 2.4787,\n",
    "                   8.6037, 12.8713, 10.4906, 5.6766])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa94605",
   "metadata": {},
   "source": [
    "We want to test whether the clammy score is useful in explaining\n",
    "the psychopathy data, over and above the students’ college affiliation.\n",
    "\n",
    "To do this, we will use an [F test](https://textbook.nipraxis.org/hypothesis_tests.html#f-tests).\n",
    "\n",
    "An F-test compares a *full model* $\\Xmat_f$ with a *reduced model* $\\Xmat_r$.\n",
    "\n",
    "In our case, $\\Xmat_f$ is the model containing the `clammy` regressor, as\n",
    "well as the two dummy columns for the UCB and MIT group means.\n",
    "\n",
    "$\\Xmat_r$ is our original model, that only contains the dummy columns for the\n",
    "UCB and MIT group means.\n",
    "\n",
    "We define $SSR(\\Xmat_r)$ and $SSR(\\Xmat_f)$ as in [hypothesis tests](https://textbook.nipraxis.org/hypothesis_tests.html).\n",
    "These are the Sums of Squares of the Residuals of the reduced and full model\n",
    "respectively.\n",
    "\n",
    "$$\n",
    "\\bhat_r = \\Xmat_r^+ \\yvec \\\\\n",
    "\\hat\\evec_r = \\yvec - \\Xmat_r \\bhat_r \\\\\n",
    "SSR(\\Xmat_r) = \\hat\\evec_r^T \\hat\\evec_r \\\\\n",
    "\\\\\n",
    "\\bhat_f = \\Xmat_f^+ \\yvec \\\\\n",
    "\\hat\\evec_f = \\yvec - \\Xmat_f \\bhat_f \\\\\n",
    "SSR(\\Xmat_f) = \\hat\\evec_f^T \\hat\\evec_f\n",
    "$$\n",
    "\n",
    "You can calculate the F statistic for adding the `clammy` regressor, by\n",
    "using these calculations and the formula for the F-test in [F tests](https://textbook.nipraxis.org/hypothesis_tests.html#f-tests).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-language_info",
   "split_at_heading": true,
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
